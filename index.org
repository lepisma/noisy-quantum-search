#+TITLE: Noisy Quantum Search
#+SETUPFILE: ../../../assets/export.setup

#+LATEX_CLASS: article
#+LATEX_HEADER: \usepackage{biblatex}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage[usenames, dvipsnames]{color}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \definecolor{plotblue}{RGB}{97,159,202}
#+LATEX_HEADER: \definecolor{plotpurple}{RGB}{157,94,145}

#+BEGIN_QUOTE
This page is a work in progress
#+END_QUOTE

\begin{abstract}
We explore the effect of imperfect oracle in Grover's iterations and see how this
can be exploited to manipulate the amplitudes oscillations in time. Specifically,
we see how controlled manipulation of oracle can result in a reduced average
probability of failure. We also explore noisy oracle empirically and its possible
use for selectively picking from marked items.
\end{abstract}

* Introduction

Grover's search cite:grover1996fast is a quantum search algorithm which provides
quadratic speedup over classical search algorithms. One of the issues with
Grover's search is that, in most cases, a classical algorithm can be crafted
which exploits the domain knowledge of the search space to get a much better
performance. This renders quantum search useful for cases where we have
unstructured/unknown search space.

One suitable use case with an unstructured search space is of something like
Genetic Programming which is an application of Genetic Algorithm to /evolve/ a
computer program represented commonly in the form of a tree. The search space of
solutions, i.e. program representations solving a particular problem, here is
made up of variations of this tree structure and is not continuous. Like many
other heuristics GP is an iterative algorithm. We start with a set of possible
solutions in a /population/, evolve it at each time step and stop when we reach
a desired level of solution quality. The evolution, in general, involves
selecting a few solutions from current population based on their quality and
using them to create the next population by applying certain genetic operators.

The point to notice is that each step of evolution here gets directly translated
to a search for $M$ items in a group of $N$ which is solvable in order
$\mathcal{O}(\sqrt{N/M})$ cite:boyer1996tight. But the $N$ items in each step
might be different. Using the terminology from Grover's search, this effectively
means that the oracle is going to be different for each time step. Similar
situation, of changing oracle, also arises when we want to evaluate a fixed set
of functions on a data set which is batched, something done commonly in regular
batch based training in machine learning.

Motivation for the exploration in this report comes from the fact that the
probability of finding a result in case of changing oracle is not intuitive in a
straightforward way. As an example, consider a case where we are trying to find
a marked item from a list of 20 in which 5 are actually /marked/ (the oracle
evaluates them as marked on each iteration), we expect equal probability for
finding any of these 5. Now consider that at some time during the Grover
iterations, the oracle didn't mark 1 of the 5 items. However innocuous this
single error might look, it breaks the symmetry (between the 1 erred item and
the rest 4) a lot depending on the time when the error was committed, see
figure 1.

#+LABEL: fig-time
#+BEGIN_EXPORT latex
\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{images/five-twenty-one-bad}
    \caption{Error at time 20}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{images/five-twenty-one-good}
    \caption{Error at time 10}
  \end{subfigure}
  \caption{Changing the time of error for one of the marked items.
In the upper panel (for {\sl AMP**2}) of each plot, \textbf{\textcolor{plotblue}{blue}} lines show the amplitudes for the marked item with error,
\textbf{\textcolor{plotpurple}{purple}} shows the amplitudes for marked items without error.}
\end{figure}
#+END_EXPORT

In this report, we try to analyze and control the /one/ error for our benefit and
look at employing randomness as a viable solution for the original problem of
selecting $m$ from $n$ proportional to quality.

\S 2 lays out the problem and its analysis in a simple setting where an oracle
makes a single error for /half/ of the true marked items. Next section (\S 3)
presents some empirical results with a noisy oracle. \S 4 concludes the report.

* Imperfect oracle

The problem we work with is of finding a /marked/ item in a set of $N$ items. It
is known that there are $M$ ($< N$) marked items in these $N$. Marked items are
identified by invoking an oracle which has a boolean return type. What we want
is to have a strategy to find one of the marked items with least number of
oracle invocations.

Now, the oracle in our definition can be represented as a function taking in the
index of an item $i$ ($i$ goes from $0$ to $N-1$), and an identifier for current
time $t$. Having the time information effectively gives a sense of state to the
oracle and lets it manipulate the output its going to provide. From now on, we
will refer to the count of marked items affected by $i$ errors (in a fixed $T$
steps of Grover's iterations) from the oracle as $M(i)$. In the case of perfect
oracle, $M(0) = M$ and $M(i) = 0$ $\forall i > 0$

In this section, we try to see what happens when the oracle makes a mistake at a
certain time $t$ while marking some of the $M$ s. The oracle keeps on behaving
like a perfect oracle after this and doesn't make more mistakes. This means that
we will have two type of marked nodes, $M(0)$ and $M(1)$ (and $M(0) + M(1) =
M$).

Since the evolution of probabilities in Grover's search is dependent on the mean
at a given time, this gives us interesting behavior depending on the time of
error in an imperfect oracle. Other than adding a gap in the curves of $M(0)$
and $M(1)$, it also helps in reducing the mean probability of failure, i.e. the
probability of finding the unmarked $N - M$ nodes reduces. In the next
subsection, we try to find the optimal time for an oracle to make mistakes
(assuming there is just one time step available for erring) so that the mean
probability of failure is minimum afterwards. We work with expressions from
cite:biham1999grover where the authors provide ways to understand Grover's
iterations with arbitrary initial amplitudes. The toy situation we use is of a
case with $M(1) = M(0) = M/2$ which is easy to analyze as it leads to a lot of
cancellations.

** Error in $M/2$ items

Consider the usual setting of $N$ items, $M$ of which are marked. The aim of the
search is to find a marked item with high probability. We consider two cases
which differ in what happens at a certain time $t$:

1. *Perfect oracle*: The oracle marks all $M$ marked items as usual at $t$.
2. *Imperfect oracle*: The oracle only marks $M/2$ items from the set of $M$
   marked items at $t$.

In the next steps ($\ge t + 1$) the oracles behaves properly and marks all $M$
marked items. Let $k(t)$ denote the amplitude of a marked item and $k'(t)$
denote the amplitude of marked item which will be affected by an oracle error.
Also let $l(t)$ denote the amplitude of each unmarked state at time $t$. In the
current setting, all $N - M$ unmarked items will have same $l(t)$, all $M/2$
error affected items will have same amplitude $k'(t)$ (different than $k(t)$)
and the rest unaffected $M/2$ items will have amplitudes $k(t)$ each.

We want to see how the average probability of success changes as we change the
time of error. Using the results from cite:biham1999grover, we have the
expression for average probability of success following the regular Grover
iterations starting with arbitrary amplitudes at time $t = 0$:

\begin{align*}
  P_{av} &= 1 - (N - M) \sigma_l^2 - \frac{1}{2} [ (N - M) \vert \bar{l}(0) \vert^2 + M \vert \bar{k}(0) \vert^2 ] \\
\end{align*}

Here $\sigma_l^2$ is the variance in amplitudes of unmarked items at time 0.
$\bar{l}(0)$ denotes the average amplitude of /unmarked/ items at time 0 and
$\bar{k}(0)$ denotes the average amplitude of /marked/ items at time 0. Let's also
define $C(t)$ as:

\begin{align*}
  C(t) = \frac{2}{N} [(N - M)\bar{l}(t) - M\bar{k}(t)]
\end{align*}

Using $C(t)$ the dynamics of the amplitudes can be given by:

\begin{align*}
  k(t + 1) &= C(t) + k(t) &\text{for marked items} \\
  l(t + 1) &= C(t) + l(t) &\text{for unmarked items}
\end{align*}

For comparing $P_{av}$ for the two cases, we will manipulate item amplitudes as
follows:

1. At time $t - 1$ (and times before that), the system follows the regular
   Grover's iteration starting at $t=0$ from a uniform superposition over
   states.
2. At time $t$, the oracle outputs incorrect value for $M/2$ marked items in the
   imperfect setting. The perfect oracle goes on as usual.
3. At time $t + 1$, both oracles are perfect.

Starting with time $t$, we will now prepare amplitudes of the marked and
unmarked items at time $t + 1$ for both cases (perfect and imperfect oracle)
which will then be used in the expression for $P_{av}$ as the /zero time/. Since
at time $t + 1$ all unmarked items will have the same amplitudes (because the
set of unmarked items is not partitioned and manipulated separately in any
step), the value $\sigma_{t + 1}^2$ will be 0 for both the cases. Thus, the
expression for $P_{av}$ we will be working with is:

\begin{align*}
  P_{av} &= 1 - \frac{1}{2} [ (N - M) \vert \bar{l}(t + 1) \vert^2 + M \vert \bar{k}(t + 1) \vert^2 ] \\
\end{align*}

*** Perfect oracle

At time $t$, the average amplitudes of the marked and unmarked states are:

\begin{align*}
  \bar{k}(t) &= k(t) \\
  \bar{l}(t) &= l(t)
\end{align*}

$C(t)$ here is:

\begin{align*}
  C(t) = \frac{2}{N} [(N - M)\bar{l}(t) - M\bar{k}(t)]
\end{align*}

Using $C(t)$, the amplitudes at time $t + 1$ can be written as:

\begin{align*}
  k(t + 1) &= C(t) + k(t) \\
  l(t + 1) &= C(t) - l(t)
\end{align*}

The amplitude averages for marked and unmarked states at time $t + 1$ can now be
given as:

\begin{align*}
  \bar{k}(t + 1) &= C(t) + \bar{k}(t) = \frac{2(N - M)l(t) + (N - 2M)k(t)}{N} \\
  \bar{l}(t + 1) &= C(t) - \bar{l}(t) = \frac{(N - 2M)l(t) - 2Mk(t)}{N} \\
\end{align*}

Now $P_{av|p}$ (for /perfect/ oracle) can be written as:

\begin{align*}
  P_{av|p} &= 1 - \frac{1}{2} [ (N - M) \vert \bar{l}(t + 1) \vert^2 + M \vert \bar{k}(t + 1) \vert^2 ] \\
  &= 1 - \frac{1}{2N^2} [ (N - M) ((N - 2M)l(t) - 2Mk(t))^2 + M (2(N - M)l(t) + (N - 2M)k(t))^2 ] \\
  &= 1 - \frac{1}{2} [(N - M) l(t)^2 + M k(t)^2]  \\
  &= \frac{1}{2} \\
\end{align*}

The last equality follows because sum of all squared amplitudes (probabilities) equals 1.

*** Imperfect oracle

Analyzing the second case (with error at time $t$), we get the average
amplitudes of the marked states as:

\begin{align*}
  \bar{k}(t) &= k(t) = k'(t)
\end{align*}

For unmarked states (note that we have $M/2$ more unmarked states):

\begin{align*}
  \bar{l}(t) &= \frac{(N - M) l(t) + (M/2) k(t)}{N - (M/2)} \\
  &= \frac{2(N - M) l(t) + Mk(t)}{2N - M}
\end{align*}

Now, $C(t)$ can be given as:

\begin{align*}
  C(t) &= \frac{2}{N} [(N - (M/2)) \bar{l}(t) - (M/2) \bar{k}(t)] \\
  &= \frac{2}{N} [(N - M) l(t) + (M/2) k(t) - (M/2) k(t)] \\
  &= \frac{2(N - M)}{N} l(t)
\end{align*}

Using $C(t)$, the amplitudes at time $t + 1$ can be written as:

\begin{align*}
  k(t + 1) &= C(t) + k(t) \\
  k'(t + 1) &= C(t) - k'(t) = C(t) - k(t) \\
  l(t + 1) &= C(t) - l(t)
\end{align*}

For calculating the average amplitudes for marked and unmarked states notice
that the grouping of marked and unmarked items is reverted to the original state
now. Therefore,

\begin{align*}
  \bar{k}(t + 1) &= C(t) \pm k(t) = \frac{2(N - M)}{N} l(t) \\
  \bar{l}(t + 1) &= C(t) - l(t) \\
  &= \frac{2(N - M)}{N} l(t) - l(t) \\
  &= \frac{N - 2M}{N} l(t) \\
\end{align*}

In this case, $P_{av|i}$ (for /imperfect/ oracle) is:

\begin{align*}
P_{av|i} &= 1 - \frac{1}{2} [ (N - M) \vert \bar{l}(t + 1) \vert^2 + M \vert \bar{k}(t + 1) \vert^2 ] \\
&= 1 - \frac{(N - M)}{2}l(t)^2 \\
&= P_{av|p} + \frac{Mk(t)^2}{2}
\end{align*}

Reading these results, if we want to reduce the mean probability of error, the
best strategy is to make errors at the time when $k(t)^2$ is the maximum. An
example of such situation is shown in figure 2.

#+LABEL: fig-perfect
#+BEGIN_EXPORT latex
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/two-twenty-one-good}
  \caption{Erring at the right time can reduces the probability of failure to be
near zero for rest of the iteration.}
\end{figure}
#+END_EXPORT

In a way, this provides an alternative to find $M$ items if we have such a
control over the oracle. We run Grover's iterations up to the time when $k(t)^2$
is maximum ($\mathcal{O}(\sqrt{N/M})$) and then make errors from the oracle's
side to reach this zone of high success probability regardless of when the
measurement is made.

* Noisy Oracle

A more practical scenario is when we don't know $M$ or have that much control
over the oracle. This also is connected to the case of the original problem
where we have oracle changes are dependent on either changing functions (items)
or changing data stream (which is used by oracle for evaluating items). A plot
of this case with noisy oracle is shown in figure 3.

#+LABEL: fig-random
#+BEGIN_EXPORT latex
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/five-twenty-noise}
  \caption{Oracle with 0.2 noise}
\end{figure}
#+END_EXPORT

Turns out, randomization does, on an average decrease the probability of failure
and has an interesting effect of working as the /damping ratio/ for the
oscillations in the Grover's iteration. See figure 4.

#+LABEL: fig-damping
#+BEGIN_EXPORT latex
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/trace-all}
  \caption{Effect of noise parameter on mean probabilities over 10000 trials}
\end{figure}
#+END_EXPORT

An interesting question is of how varying the noise for each $M$ affects the
probability of picking among the $M$. Simulating this situation and tracing the
mean curves creates a set of damped oscillation curves as shown in figure 5.
Other than the convergence behavior, an important point to notice is the zone
where the $M$ items are separable depending on the probability of error.

#+LABEL: fig-varying
#+BEGIN_EXPORT latex
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/trace-varying}
  \caption{Mean trace (over 10000 trials) of probabilities for finding 5 items
(among 20) with different noise value for each item}
\end{figure}
#+END_EXPORT

* Conclusions and Future Work

We see that the behavior of Grover's iterations in case of an imperfect oracle
is dependent on the time of error. More generally the effect of error at time
$t$ depends on the amplitudes at that instant and this can fact can be used to
manipulate future amplitudes and, in turn, the probability of success.

Another interesting finding is that a noisy oracle, considering the mean over a
number of trials, /does/ perform decently enough to both reduce the probability
of error /and/ provide a way to differentiate between $M$ items.

Looking forward, some other questions are:
- What is the exact form for the damping behavior of noise parameter, both in
  the independent case and in the situation with varying noise per item in the
  set of $M$?
- Hoe can the asymmetry induced by errors be used to selectively pick among the
  M items?
- Although the probabilities can be manipulated using a clever oracle, this
  might end up taking enough effort to mute the advantages. How feasible is it
  to build a controllable oracle?

bibliography:./references.bib

* Scratchpad                                                       :noexport:
- What about meta search over oracle /and/ data

** QuIDD
You can actually simulate quantum computer using polynomial (instead of
exponential) slowdown so the grover search thing looks a bit muddier now.

** Search and generalization
- Positive examples create more generalizations
- Negative examples prune some of them

Version Space Learning has certain good ideas about how to do hypothesis
searches. There are two approaches, either your stream in the instances and
narrow down your lense on the hypothesis space. Or you use a /generate-and-test/
type technique. There is also a possibility of mixing both of these here.
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.5764&rep=rep1&type=pdf

** Some Quantum GA papers
- A quantum genetic algorithm with quantum crossover and mutation operations
  https://arxiv.org/pdf/1202.2026.pdf
- Quantum Genetic Optimization
  https://www.uni-ulm.de/fileadmin/website_uni_ulm/nawi.inst.220/publikationen/04358783.pdf

** Dynamic Grover search
Paper is here https://arxiv.org/pdf/1505.00895.pdf. Have to read to find out
what exactly they want.

** Other search (spatial/hypercube) papers
- Quantum Search of Spatial Regions
  https://www.scottaaronson.com/papers/ggtoc.pdf
- Quantum Walk based search algorithm https://www.irif.fr/~santha/Papers/s08.pdf
- Spatial search by Quantum Walk https://arxiv.org/abs/quant-ph/0306054
- A Quantum Random Walk Search Algorithm
  https://arxiv.org/pdf/quant-ph/0210064.pdf
- Review on Quantum Search Algorithms https://arxiv.org/pdf/1602.02730.pdf
- Is q search practical.
  https://web.eecs.umich.edu/~imarkov/pubs/misc/iwls04-antiGrover.pdf
